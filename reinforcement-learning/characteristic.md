# 자연어 생성에서 강화학습 적용하기

강화학습은 마코프 결정 프로세스(Markov Decision Process, MDP)상에서 정의되고 동작합니다. 따라서 여러 선택(action)들을 통해서 여러 상황(state)들을 옮겨다니며(transition) 에피소드가 구성되고, 선택된 행동과 상태에 따라서 보상(reward)이 주어지며 누적(cumulated)되어, 에피소드가 종료되면 누적보상(cumulative reward)를 얻을 수 있습니다.

따라서 이를 자연어처리에 적용하게 되면 텍스트 분류(text classification)와 같은 문제에 적용되기 보단, 시퀀셜 데이터를 예측해야 하는 자연어생성(natural language generation)에 적용되게 됩니다. 이제까지 생성된 단어들의 시퀀스가 현재의 상황(current state)이 될 것이며, 이제까지 생성된 단어를 기반으로 새롭게 선택하는 단어가 행동이 될 것 입니다. 이렇게 문장의 첫 단어(BOS, beginning of sentence)부터 문장의 끝 단어(EOS, end of sentence)까지 선택하는 과정(한 문장을 생성하는 과정)이 하나의 에피소드(episode)가 됩니다. 우리는 훈련 코퍼스에 대해서 문장을 생성하는 경험을 축적하여(즉, episode를 반복하여) 실제 정답과의 비교를 통해 기대누적보상(expected cumulative reward)을 최대화(maximize)할 수 있도록 sequence-to-sequence 네트워크 파라미터(강화학습에서는 정책망) $\theta$ 를 훈련 하는 것 입니다.

![강화학습을 기계번역에 적용한다면](image_needed)

구체적으로 기계번역에 강화학습을 대입시켜 보면, 현재의 상태(state)는 주어진 소스(source) 문장과 이전까지 생성(번역)된 단어들의 시퀀스가 될 것이고, 행동을 선택하는 것을은 현재 상태에 기반하여 새로운 단어를 선택 하는 것이 될 것 입니다. 그리고 현재 time-step의 행동을 선택하게 되면 다음 time-step의 상태는 소스 문장과 이전까지 생성된 단어들의 시퀀스에 현재 time-step에 선택된 단어가 추가되어 정해지게 됩니다. 중요한 점은 행동을 선택한 후에 환경(environment)으로부터 즉각적인(immediate) 보상(reward)을 받지는 않으며, 모든 단어의 선택이 끝나고, 최종적으로 EOS를 선택하여 디코딩이 종료되어 에피소드(episode)가 끝나면, BLEU 점수를 계산하여 누적 보상을 받을 수 있습니다. 즉, 종료 시에 받는 보상값은 에피소드 누적보상값(cumulative reward)과 일치 합니다.

강화학습을 통해 모델을 훈련할 때, 훈련의 처음부터 강화학습만 적용하기에는 그 훈련방식이 비효율적이고 어려움이 크기 때문에, 보통 기존의 MLE를 통해 어느정도 학습이 된 신경망 $\theta$ 에 강화학습을 적용 합니다. <comment> 강화학습은 탐험(exploration)을 통해 더 나은 정책의 가능성을 찾고, 착취(exploitation)를 통해 그 정책을 발전시켜 나갑니다. </comment>

## 자연어 생성에서의 강화학습의 특성

우리는 이제까지 강화학습 중에서도 정책기반 학습 방식인 폴리시 그래디언트에 대해서 간단히 다루어 보았습니다. 사실, 폴리시 그래디언트의 경우에도 소개한 방법 이외에도 발전된 방법들이 많이 있습니다. 예를 들어 액터크리틱(Actor Critic)의 경우에는 정책망 $\theta$ 이외에도 가치 네트워크 $W$ 를 따로 두어, 에피소드의 종료까지 기다리지 않고 TD 학습법을 통해 학습이 가능합니다. 여기에서 더욱 발전하여 기존의 단점을 보완한 A3C와 같은 다양한 방법들이 존재 합니다.

하지만, 자연어처리에서의 강화학습은 이런 다양한 방법들을 굳이 사용하기보다는 간단한 REINFORCE를 사용하더라도 큰 문제가 없습니다. 이것은 자연어처리 분야의 특성에서 기인합니다. 강화학습을 자연어처리에 적용할 때는 아래와 같은 특성들이 있습니다.

1. 선택 가능한 매우 많은 행동 $a_t$ 가 존재 합니다. 보통 다음 단어를 선택하는 것이 행동(action)을 선택 하는 것이 되기 때문에, 선택 가능한 행동의 집합의 크기는 어휘(vocabularty) 사전의 크기와 같다고 볼 수 있습니다. 따라서 그 집합의 크기는 보통 몇만개가 되기 마련입니다.
1. 매우 많은 상태(state)들이 존재 합니다. 단어를 선택하는 것이 행동이었다면, 이제까지 선택된 단어들의 시퀀스는 상태가 되기 때문에, 여러 time-step을 거쳐 수많은 행동(단어)들이 선택되었다면, 가능한 상태의 경우의 수는 매우 커질 것 입니다.
1. 따라서 매우 많은 행동들을 선택하고, 매우 많은 상태들을 훈련 과정에서 모두 겪어보는 것은 거의 불가능하다고 볼 수 있습니다. 결국 추론(inference)과정에서 보지 못한(unseen) 샘플을 만나는 것은 매우 당연한 일이 될 것 입니다. 따라서 이러한 희소성(sparseness)문제는 큰 골칫거리가 될 수 있습니다. 하지만 우리는 딥러닝을 통해서 이 문제를 해결 할 수 있습니다.
1. 강화학습을 자연어처리에 적용할 때에 쉬운 점도 있습니다. 대부분 하나의 문장을 생성하는 것이 하나의 에피소드(episode)가 되는데, 보통 문장의 길이는 길어봤자 100 단어 미만 일 것 입니다. 따라서 다른 분야의 강화학습보다 훨씬 쉬운 이점을 가지게 됩니다. 예를 들어 딥마인드(DeepMind)의 바둑(AlphaGo)이나 스타크래프트의 경우에는 하나의 에피소드가 끝나기까지 매우 긴 시간이 흐르게 됩니다. 따라서 에피소드 내에서 선택되었던 행동들이 정책을 업데이트 하기 위해서는 매우 긴 에피소드가 끝나기를 기다려야 할 뿐만 아니라, 10분 전에 선택 했던 행동이 이 게임(game)의 승패에 얼마나 큰 영향을 미쳤는지 알아내는 것은 매우 어려운 일이 될 것 입니다. 따라서, 자연어처리 분야가 다른 분야에 비해서 에피소드가 짧은 것은 매우 큰 이점으로 작용하여 정책망(policy network)을 훨씬 더 쉽게 훈련 시킬 수 있게 됩니다.
1. 대신에 문장 단위의 에피소드를 가진 강화학습에서는 보통 에피소드 중간에 보상을 얻기는 힘듭니다. 예를 들어 번역의 경우에는 각 time-step 마다 단어를 선택할 때 즉각적인 보상을 얻지 못하고, 번역이 모두 끝난 이후 완성된 문장과 정답(reference) 문장을 비교하여 BLEU 점수를 누적 보상으로 사용하게 됩니다. 마찬가지로 에피소드가 매우 길다면 이것은 매우 큰 문제가 되었겠지만, 다행히도 문장 단위의 에피소드 상에서는 큰 문제가 되지 않습니다.

## 강화학습을 통해 얻을 수 있는 장점

이제까지 우리는 강화학습에 대해서 간단히 이야기하고, 이를 우리의 자연어생성 문제에 적용하는 방법에 대해서 다뤘습니다. 강화학습을 적용하여 성능을 개선할 수 있게하는 장점은 무엇이 있을까요? 다시 정리해보도록 하겠습니다.

### Teacher Forcing의 대체재

이전에 우리는 sequence-to-sequence와 같은 auto-regressive한 속성을 가진 모델을 훈련하기 위해서는 teacher forcing이라는 방법을 사용한다고 하였습니다. 따라서 훈련방식과 추론방식의 차이가 생겨서, 실제 추론하는 방식과 다르게 모델을 훈련할 수 밖에 없었습니다. 한마디로 시험을 보는 방식과 다르게 모델에게 가르쳐주고 더 나은 성능을 기대하는것과 같은 모습 입니다.

하지만 이제 강화학습을 통해 실제 추론하는 형태와 같이 샘플링을 통해 모델의 학습을 진행할 수 있게 되었습니다. 즉, 학습 방법과 추론 방법의 차이가 없습니다.

### 더 정확한 목적함수의 사용

이전 챕터에서 BLEU에 대해 다루면서, perplexity(PPL)에 비하여 더 나은 번역의 품질을 반영한다고 설명하였습니다. 사람의 언어는 어순이나 의미의 모호성등 복잡한 구조와 형태를 띄고 있기 때문에, 단순한 PPL과 같은 metric(매트릭)은 정확한 평가 지표가 되기 힘듭니다. 따라서 번역 이외에도 여러가지 자연어처리 분야에는 다양한 평가 지표가 존재 합니다. 이 평가방법들은 그래디언트를 구하기 불가능한 경우가 대부분이기 때문에, 뉴럴네트워크를 훈련하는데는 사용 될 수 없었습니다. 

하지만 강화학습의 폴리시 그래디언트를 응용하여 보상함수에 대해 미분을 계산할 필요가 없어지면서, 정확한 평가지표의 사용이 가능해졌습니다.
