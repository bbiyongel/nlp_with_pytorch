# 정리

이번 챕터에서는 강화학습에 대해서 이야기하고, 강화학습을 통해 자연어 생성 문제를 해결하는 방법에 대해서 다루었습니다. 강화학습은 그 자체만으로도 우리가 이 책에서 다루기에는 매우 방대하고 깊은 학문입니다. 물론 다양한 강화학습 알고리즘을 사용하여 자연어 생성 문제에 대한 성능을 높일 수 있겠지만, 우리는 그 중에서도 폴리시 그래디언트를 사용하여 자연어 생성에 적용 하는 방법을 이야기 하였습니다.

폴리시 그래디언트 방법을 자연어 생성에 적용함으로써 얻을 수 있는 이점은 크게 두 가지로 정리할 수 있습니다. 첫 번째, auto-regressive 속성으로 인해서 실제 추론 방식과 다르게 훈련해야 하는 teacher-forcing 훈련 방법을 탈피하여, 실제 추론 방식과 같은 샘플링을 통해서 문장 생성 능력을 높일 수 있습니다. 두 번째, 좀 더 정확한 목적함수에 대해서 훈련할 수 있게 되었습니다. 기존의 PPL은 번역 또는 문장의 생성 품질을 정확하게 반영할 수 없는 단점을 갖고 있었고, 때문에 BLEU 또는 기타 여러가지 메트릭(metric)을 사용하여 모델에 대한 성능을 측정할 수 있었습니다. 하지만 BLEU와 같은 평가 함수는 미분을 할 수가 없었기 때문에, PPL과 동일한 크로스 엔트로피를 활용하여 우리는 뉴럴 네트워크를 훈련 할 수 밖에 없었습니다. 폴리시 그래디언트는 보삼함수에 미분을 할 필요가 없기 때문에, 이 점을 활용하면 어떤 보상 함수든지 활용하여 뉴럴 네트워크를 훈련 할 수 있게 되는 것 입니다.

하지만 폴리시 그래디언트에도 단점이 있습니다. 일단 샘플링을 기반으로 훈련을 하기 때문에, 많은 반복 시행이 필요합니다. 따라서 훈련에 훨씬 많은 시간이 소요되어 좀 더 비효율적인 학습이 진행 됩니다. 또한 보상함수는 스칼라(scalar)값을 리턴하기 때문에, 보상함수를 최대화 하는 방향을 정확하게 알 수 없습니다. 이는 기존의 MLE 방식에서 손실 함수를 뉴럴 네트워크 파라미터 $\theta$ 에 대해서 미분하여 얻은 그래디언트를 통해서 손실 함수 자체를 최소화 하는 방향으로 업데이트 하던 것과 차이가 있습니다. 결국, 이 또한 기존의 MLE 방식에 비해서 훨씬 비효율적인 학습을 진행하게 만듭니다.
