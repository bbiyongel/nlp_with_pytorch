# 정리

이번 장에서는 전이학습(transfer learning)을 활용하여 자연어 처리 기술의 성능을 끌어올리는 방법에 대해 살펴보았습니다. 기존의 워드투벡터(word2vec)와 같은 사전훈련(pretraining) 방법과 달리 이번 장에서 다룬 내용들은 ELMo에서처럼 문맥을 반영한 단어 임베딩 벡터를 구하거나, BERT와 같이 신경망 전체에 대해서 사전 학습된 웨이트 파라미터를 제공합니다.

특히, 기존의 영상처리(computer vision) 분야에서 사전에 이미지넷(ImageNet) 데이터를 통해 훈련된 웨이트 파라미터를 다른 데이터셋의 훈련에 사용함으로써 얻었던 효과를, 이제 자연어 처리 분야에서도 얻을 수 있게 되었습니다. BERT는 손쉽게 수집이 가능한 일반적인 문장들을 바탕으로 양방향 언어모델을 학습한 후, 이를 다른 문제 해결에 사용합니다. 따라서, 예를 들어 기계번역과 같이 병렬 코퍼스가 필요하거나, 텍스트 분류와 같이 레이블링(labeling)된 코퍼스가 필요한 경우에 제한적인 데이터셋 수집이 될 수 밖에 없지만, 전이학습을 통해서 훨씬 더 많은 양의 코퍼스로부터 문장에서 특징을 추출하는 방법을 신경망이 배울 수 있을 것 입니다.

이처럼 이제 우리는 BERT와 같은 하나의 모델 아키텍쳐를 통해서 다양한 분야의 자연어 처리 문제들에서 매우 높은 성능을 달성할 수 있게 되었습니다.

## 한계

하지만 여전히 우리가 원하는 수준의 인공지능에 도달하기 위해서는 여러가지 어려움이 남아있습니다. 먼저, 아직 전이학습에 대한 명확한 수학 및 이론적인 배경이 정립되지 않았습니다. 따라서, 우리는 이와 같은 전이학습을 구성하는데 있어서 상당히 경험적(empirically)으로 접근할 수 밖에 없는 것이 사실입니다. 물론 경험적인 접근을 통해서도 여전히 기존에 비해 매우 높은 성능을 달성할 수 있기 때문에 훌륭하지만, 만약 이론적인 내용이 좀 더 탄탄해진다면 훨씬 더 높은 성능을 달성할 수도 있을 것이기 때문 입니다.

또한 비록 BERT의 활용을 통해 우리는 손쉽게 사람보다 뛰어난 질의응답(question answering) 문제 해결능력을 가진 신경망을 얻을 수 있게 되었지만, 그렇다고해서 그 신경망이 정말 사람보다 뛰어난 두뇌를 가진 인공지능이 되는 것은 아닙니다. 사람들은 대화할 때, 서로가 가진 공통된 지식이나 상식등을 통해 쉽게 생략된 정보를 유추하고 문맥을 이해합니다. 하지만, 아직 딥러닝에게는 이러한 지식이나 상식을 배워 자연어 처리 문제에 적용하고 해결하는 능력은 없습니다. 단지 주어진 문장들만을 활용하여 단순한 질의응답 문제를 해결하는 것일 뿐 입니다. 그러므로 마치 영화에서처럼 자유자재로 사람과 대화하는 인공지능을 만들기 위해서는 아직 풀어야 하는 숙제가 많이 남아 있습니다.
