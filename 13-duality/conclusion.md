# 정리

이번 챕터에서는 기존의 강화학습을 넘어서 듀얼리티라는 속성을 활용하여 기계번역의 성능을 극대화 하는 방법에 대해서 살펴 보았습니다. 이전 챕터에서 언급하였듯이, 폴리시 그래디언트 방식은 미분 불가능한 보상함수도 사용할 수 있는 대신, 샘플링 기반으로 동작하기 때문에 훨씬 더 비효율적인 학습을 진행해야 했습니다. 하지만 듀얼리티를 활용한 방식에서는 기존의 MLE 및 teacher-forcing 방식 아래에서, teacher-forcing 방식의 단점을 보완하기 위한 regularization 텀(term)을 추가함으로써, 모델의 성능을 극대화 하였습니다.

또한 기존의 back-translation 등의 단방향 코퍼스 활용 방법에 대한 재해석을 제공합니다. 더군다나 통계에 기반하여 해석이 훨씬 더 수월하기 때문에, 현재 딥러닝 학계의 연구방향과 많은 부분에서 일치하므로, 앞으로 발전 가능성이 좀 더 많다고 볼 수 있습니다. 이처럼 다양한 방법을 통해서 기존의 sequence-to-sequence 방식에서의 단점을 보완하려는 시도들이 있었으며, 이를 통해 우리는 한발 더 자연어 생성 문제 해결에 다가설 수 있었습니다.
