# 정리

이번 챕터에서 우리는 sequence-to-sequence와 어텐션에 대해서 다루었습니다. 이 구조는 시퀀셜 데이터를 입력으로 받아서 시퀀셜 데이터를 출력으로 내어주는 모델 입니다. 기본적으로 2개의 RNN으로 이루어져 있으며, LSTM을 사용하더라도 기억할 수 있는 길이에는 한계가 있기 때문에, 어텐션을 통해서 그 한계를 극복한 것을 이야기 하였습니다.

또한 자연어 생성 문제에서 비롯된 auto-regressive 속성 때문에 추론 방법에서 벗어난 훈련 방법인 teacher forcing에 대해서도 이야기 하였습니다. 추후 이어지는 챕터들에는 이러한 teacher forcing으로 인해 생긴 문제들을 해결하기 위한 방안들에 대해서 소개하도록 합니다.

그리고 번역과 같은 자연어 생성 문제일때 평가 방법에 대해서 다루었습니다. 비록 PPL이 확률값을 길이로 정규화하여 문장의 길이에 상관없이 문장의 유창성을 판단할 수 있었지만, 번역과 같은 자연어 생성 문제에서는 PPL이 정확한 결과의 품질을 반영하기는 어렵습니다. 따라서 BLEU라는 평가 방법을 통해 우리는 좀 더 정확한 번역 품질을 얻을 수 있습니다.

이처럼 우리는 이번 챕터에서 굉장히 많은 것들을 이야기 하였습니다. 사실 이번 챕터의 내용만으로 기존 통계기반 기계번역(SMT)를 능가하는 번역 시스템을 만들어낼 수 있을 정도 입니다. 하지만 여기에서 만족하지 않고 이어지는 챕터들에서는 더욱 번역(또는 자연어생성) 문제의 성능을 끌어올리는 내용을 이야기 하도록 하겠습니다.
