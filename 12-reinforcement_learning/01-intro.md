# 강화학습을 활용한 자연어생성

이번 챕터에서 우리는 방대한 영역의 강화학습에 대해서 간단하게 이야기 하고, 그 방대한 영역의 일부분인 폴리시 그래디언트를 활용하여 자연어생성의 성능을 끌어올리는 방법에 대해서 다루고자 합니다. 본격적으로 강화학습에 대해 소개하기에 앞서, 왜 강화학습이 자연어생성에 필요한지 좀 더 이야기하고자 합니다.

## 적대적 생성 네트워크 (Generative Adversarial Network, GAN)

2016년부터 주목받기 시작하여 2017년에 가장 큰 화제였던 분야는 단연 적대적 생성 네트워크(GAN)라고 말할 수 있습니다. Variational Auto Encoder(VAE)와 함께 Generative learning을 대표하는 방법 중에 하나입니다. GAN을 통해서 우리는 사실같은 이미지를 생성해내고 합성해내는 일들을 딥러닝을 통해 할 수 있게 되었습니다. 이러한 합성/생성 된 이미지들을 통해, 자율주행과 같은 실생활에 중요하고 어렵지만 훈련 데이터셋을 얻기 힘든 문제들을 해결 하는데 큰 도움을 얻을 수 있으리라고 예상 됩니다. <comment> 실제로 GTA게임을 통해 자율주행을 훈련하려는 시도는 이미 유명합니다. </comment>

![GAN의 전형적인 구조](../assets/12-01-01.png)

$$\min_{G}\max_{D}\mathcal{L}(D,G)=\mathbb{E}_{\text{x}\sim p_r(\text{x})}\Big[\log{D(\text{x})}\Big]+\mathbb{E}_{\text{z}\sim p_z(\text{z})}\Big[\log{\big(1-D(G(\text{z}))\big)}\Big]$$

위와 같이 제너레이터(Generator) $G$ 와 디스크리미네이터(Discriminator) $D$ 2개의 모델을 각기 다른 목표를 가지고 동시에 훈련시킵니다. $D$ 는 임의의 이미지를 입력으로 받아 이것이 실제 존재하는 이미지인지, 아니면 합성된 이미지인지 탐지 해 내는 역할을 합니다. $G$ 는 어떤 이미지를 생성 해 내되, $D$ 를 속이는 이미지를 만들어 내는 것이 목표입니다. 이렇게 두 모델이 잘 균형을 이루며 min/max 게임을 펼치게 되면, $G$ 는 결국 훌륭한 이미지를 합성 해 내는 제너레이터가 됩니다.

### 왜 GAN이 중요한가?

![L1 손실함수와 GAN의 생성 결과물 비교 (출처: pix2pix)](../assets/12-01-02.png)

마찬가지의 이유로 GAN또한 주목받게 됩니다. 예를 들어, 생성된 이미지와 정답 이미지 간의 차이를 비교하는데 MSE(Mean Square Error)방식을 사용하게 되면, 결국 이미지는 MSE를 최소화 하기 위해서 자신의 학습했던 확률 분포의 중간으로 출력을 낼 수 밖에 없습니다. 예를 들어 사람의 얼굴을 일부 가리고 가려진 부분을 채워 넣도록 훈련한다면, MSE 손실함수(loss function) 아래에서는 각 픽셀마다 가능한 확률 분포의 평균값으로 채워 질 겁니다. 이것이 MSE를 최소화 하는 길이기 때문입니다. 하지만 우리는 그런 흐려진 이미지를 잘 생성된 이미지라고 하지 않습니다. 따라서 사실적인 표현을 위해서는 MSE보다 정교한 목적함수(objective function)를 쓸 수 밖에 없습니다. GAN에서는 그러한 복잡한 함수를 $D$ 가 근사하여 해결한 것 입니다.

## 앗, 그렇다면?

그렇다면 번역 또는 자연어생성에도 위의 아이디어를 적용 해 보는 것도 매우 멋진 일이 될 것 같습니다. 예를 들어 크로스 엔트로피를 사용하여 바로 학습하기 보단, 실제 코퍼스에서 나온 문장인지 sequence-to-sequence에서 나온 문장인지 판별하는 디스크리미네이터 $D$ 를 두어서, sequence-to-sequence에서 나온 문장이 진짜 문장과 같아지도록 훈련하면 정말 멋질 것 같습니다.

![GAN을 sequence-to-sequence에 적용하자](../assets/12-01-03.png)

하지만 아쉽게도 이 멋진 아이디어는 곧바로 적용할 수 없습니다. Sequence-to-sequence의 결과는 discrete 확률 분포입니다. 여기서 샘플링 또는 $\text{argmax}$ 를 통해서 얻어지는 결과물은 discrete한 값일 것이고, 따라서 one-hot 벡터로 표현되어야 할 것 입니다. 하지만 이 과정은 확률적인(stochastic) 프로세스로 그래디언트를 back-propagation 할 수 없습니다. 따라서 디스크리미네이터 $D$ 가 맞추거나 속은 여부가 sequence-to-sequence $G$ 로 전달 될 수 없고, 따라서 학습이 불가능 합니다.

![샘플링 과정은 그래디언트 전달이 어렵습니다.](../assets/12-01-04.png)

## GAN과 자연어생성

위와 같이 GAN은 컴퓨터 비전(CV)분야에서 대성공을 이루었지만 자연어처리(NLP)에서는 적용이 어려웠습니다. 그 이유는 자연어 자체의 특성에 있습니다. 이미지라는 것은 어떠한 continuous한 값들로 채워진 행렬 입니다. 하지만 이와 달리 단어 또는 문장이라는 것은 discrete한 값으로써, 결국 언어라는 것은 어떠한 descrete한 값들의 순차적인 배열 입니다. 비록 우리는 뉴럴 네트워크 언어모델이나 sequence-to-sequence를 통해서 latent 공간에서는 continuous 변수로 그 값들을 치환하여 다루고 있지만, 결국 외부적으로 언어를 표현하기 위해서는 discrete 확률 분포와 변수로 나타내어집니다. 그리고 분포가 아닌 어떤 샘플로 표현되기 위해서는, discrete 확률 분포에서 샘플링을 하는 과정을 거쳐야 합니다. 이 과정은 미분이 불가능하거나 미분이 되더라도 그래디언트가 0이 되어 back-propagation을 통한 뉴럴 네트워크의 훈련이 불가 합니다.

$$y\sim P(\text{y}|x;\theta)$$

이러한 이유 때문에 $D$ 의 손실(loss)을 제너레이터 $G$ 에 전달 할 수가 없고, 따라서 적대적 학습 방법을 자연어 생성에는 적용할 수 없는 인식이 지배적이었습니다. 하지만 강화학습을 사용함으로써 적대적 학습 방식을 우회적으로 사용할 수 있게 되었습니다. <comment> [Gumbel Softmax [Jang at el.2016]](https://arxiv.org/pdf/1611.01144.pdf)과 같이 reparameterization 트릭을 이용해 이 문제를 해결하려는 시도들도 있습니다. </comment>

## 강화학습을 사용해야 하는 이유

위와 같이 GAN을 사용하기 위함 뿐만이 아니라, 강화학습은 매우 중요합니다. 어떠한 문제를 해결함에 있어서 크로스 엔트로피(cross entropy)를 쓸 수 있는 분류 문제나 continuous 변수를 다루는 MSE 등으로 정의 할 수 없는, 복잡한 목적함수들이 많이 존재하기 때문입니다. 비록 그동안 크로스 엔트로피나 MSE로 문제를 해결하였더라도 문제를 단순화 하여 접근한 것일 수도 있습니다. 우리는 이러한 문제들을 강화학습을 통해 해결하거나 성능을 더욱 극대화 할 수 있습니다. 이를 위해서 잘 설계된 보상(reward)을 사용하여 보다 복잡하고 정교한 문제를 해결 할 수 있습니다.
