# One-hot 인코딩

앞서 우리는 단어가 가지는 형태와 내부의 의미에 대해서 알아보았습니다. 단어라는 것은 어떤 개념을 표현하고 있고, 이에 따라 분류체계(taxonomy)를 갖거나 의미간에 유사도를 가집니다. 따라서 분명히 단어와 단어간에는 매우 비슷한 의미를 지닐 수도 있고, 반대의 의미를 지닐 수도 있으며, 서로 상관없는 관계일 수도 있습니다. 하지만 단어는 앞서 계속 언급하였다시피 discrete한 심볼(symbol)로 그 내부의 의미는 유사성이 있을 수 있지만, 겉 형태는 다른 경우가 많습니다. <comment> 동형이의어 제외 </comment>

우리는 이런 언어의 특성을 활용하여 머신러닝에 잘 활용하여야 합니다. 머신러닝은 보통 데이터를 기반으로 모델을 훈련하고, 새로운 데이터가 왔을 때 잘 예측(재현)할 수 있어야 합니다. 따라서 우리는 주어진 훈련 데이터에 대해서 최대한 잘 활용하여 많은 정보를 추출해 많은 것을 배울 수 있어야 합니다.

- 고양이는 좋은 반려동물 입니다.
- 강아지는 훌륭한 애완동물 입니다.

예를 들어 위와 같이 훈련 데이터가 주어졌을 때, 우리가 바라는 것은 '고양이'$\approx$'강아지', '좋은'$\approx$'훌륭한', '반려동물'$\approx$'애완동물'라는 것을 활용하여, 유사한 단어들로부터 부족한 정보를 더 배우는 것이라고 볼 수 있습니다. 이처럼 더 나은 머신러닝을 수행하기 위해서, discrete한 단어간 유사도를 효과적으로 구하는 것은 수십년동안 자연어처리분야 연구자들의 숙원이었습니다. 물론 일부 독자들도 잘 알다시피 딥러닝에 들어서며 지금은 훌륭한 워드 임베딩 방법들이 많이 존재합니다. 그 알고리즘들도 이런 필요에서 등장한 것이라고 할 수 있습니다. <comment> 우리는 딥러닝을 활용한 새로운 워드 임베딩 방법에 대해서 다음 챕터에서 다루도록 합니다. </comment>

먼저 단어를 컴퓨터가 인지할 수 있는 수치로 바꾸는 가장 간단한 방법은 벡터로 표현하는 것 입니다. 그중에서도 가장 기본적으로 단어를 벡터로 나타내는 방법은 one-hot 인코딩(encoding)이라는 방식 입니다. 이 방식은 말 그대로 단 한개의 1과 나머지 수많은 0들로 표현된 인코딩 방식을 뜻합니다. One-hot 인코딩 벡터의 차원은 보통 전체 어휘(vocabulary)의 갯수가 되며, 당연히 보통 그 숫자는 매우 큰 숫자가 됩니다. (대략 30,000~100,000)

$$
v\in\mathbb{R}^{|V|},\text{ where }v\text{ is one-hot vector and }|V|\text{ is vocabulary size.}
$$

전체 단어에 대해서 one-hot 벡터를 구성한다면 아래와 같은 형태가 될 것 입니다.

|단어|사전에서의 순서(index)|One-hot 벡터|
|-|-|-|
|...|||
|강아지|8|$0,0,0,0,0,0,0,1,0,0,0,\cdots,0$|
|개|9|$0,0,0,0,0,0,0,0,1,0,0,\cdots,0$|
|고양이|10|$0,0,0,0,0,0,0,0,0,1,0,\cdots,0$|
|구렁이|11|$0,0,0,0,0,0,0,0,0,0,1,\cdots,0$|
|...|||
|하마|20,567|$0,\cdots,0,0,0,0,0,0,0,0,0,0,1$|
|...|||

앞어 언급하였듯이 단어는 discrete한 심볼로써, discrete 랜덤변수(random variable)로써 나타내게 됩니다. Discrete 랜덤 변수는 그 값을 불연속적으로 가질 수 밖에 없습니다. 따라서 one-hot 벡터는 discrete 확률 분포(probability distribution)로부터 샘플링(sampling)한 샘플이라고 할 수 있습니다. <comment> 주로 multinoulli 확률 분포가 될 것 입니다. </comment>

![](이미지)

위처럼 사전(dictionary)내의 각 단어를 one-hot 인코딩 방식을 통해 벡터로 나타낼 수 있습니다. 이 표현 방식은 여러가지 문제점을 갖고 있습니다. 먼저 벡터의 차원이 너무 커졌습니다. 각 벡터는 단 하나의 1을 갖고, 나머지는 0으로 가득 차 있습니다. 이처럼 벡터의 많은 부분이 0으로 채워져 있는 벡터를 sparse(희소) 벡터라고 합니다. 이러한 sparse 벡터의 가장 큰 문제점은 벡터 사이의 (유사도 구하기 등의) 연산을 할 때에 결과값이 0이 된다는 것 입니다. 즉, 서로 orthogonal한 경우가 많아지게 됩니다.

$$
[0,0,\cdots,1,0]\times[0,1,0,\cdots,0]^T=0
$$

바꿔말하면 분명히 '강아지'와 '개'라는 단어는 서로 유사한 것인데 이 둘의 유사도는 0이 나올 것이고, '강아지'와 '컴퓨터'라는 단어는 상대적으로 관계가 적을 것인데 마찬가지로 이 둘의 유사도도 0이라는 똑같은 값이 나올 것 입니다. 이것은 데이터를 통해서 학습을 수행하는 기계학습의 특성상 데이터가 많을 수록 유리한데 반해, 불리하게 작용할 수 밖에 없습니다. '강아지'에 대한 데이터가 적어 일반화(generalization)하기 어려울 때, '개'와 관련된 데이터로부터 도움을 받을 수 없을 것이기 때문입니다.

## 차원의 저주(Curse of dimensionality)

또 다른 문제는 이런 sparse vector는 기계학습에 있어서 매우 큰 장벽으로 작용한다는 점 입니다. 예를 들어, 점보를 표현하는데 훨씬 큰 차원이 사용되었다면 작은 차원으로 같은 정보를 표현한 것에 비해서, 상대적으로 같은 크기의 공간에 표현되는 정보는 훨씬 더 적을 것이기 때문입니다. 아래의 그림과 같이 같은 크기로 나뉘어진 단위 공간이 비어있는 경우가 많을 것 입니다. 그리고 정보를 표현하는 각 점(벡터)들은 매우 낮은 밀도로 희소(sparse)하게 퍼져 있을 것 입니다.

![차원의 저주: 차원이 높을 수록 같은 정보를 표현하는데 불리합니다.](picture)

차원이 늘어날수록 이와 같은 문제가 지수적으로(exponential) 늘어나게 됩니다. 우리는 이런 문제를 차원의 저주(curse of dimensionailty)라고 부릅니다. 따라서 우리는 위와 같이 one-hot encoding의 한계를 실감할 수 밖에 없습니다. 따라서 우리는 차원의 저주로부터 벗어나기 위해 차원을 축소하여 단어를 표현해야 할 필요성을 느낍니다.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE5MjE3NjM2MzBdfQ==
-->