# 요약

이번 챕터에서는 단어의 의미에 대해서 다뤄보고, 의미의 유사성이나 모호성에 대해서 다루어보았습니다. 단어는 겉의 discrete한 형태와 달리 내부적으로는 non-discrete한 '의미(sense)'를 갖고 있습니다. 따라서 우리는 단어의 겉 모양이 다르더라도 의미가 유사할 수 있음을 알고 있습니다. 이렇게 의미가 유사한 단어들간의 유사도를 계산 할 수 있다면, 코퍼스(corpus)로부터 분포나 피쳐(feature)들을 훈련 할 때 좀 더 많은 정보를 얻고 정확한 훈련을 할 수 있습니다. 예를 들어 꼭 소스(source) 단어가 아니더라도 그 소스 단어와 유사한 단어들로부터 정보를 얻어올 수 있고, 그 정보를 소스 단어와 유사한 만큼만 사용하면 될 것 입니다.

따라서, 이러한 자연어처리의 염원 아래, 사람이 직접 한땀한땀 정성들여 만든 워드넷이라는 사전이 등장하게 되었고, 워드넷을 활용하여 단어 사이의 유사도(거리)를 계산할 수도 있게 되었습니다. 하지만 이렇게 워드넷과 같은 시소러스(thesaurus, 어휘 분류 사전)을 구축하는 것은 너무나도 엄청난 일이므로, 사전이 없이 유사도를 구할 수 있으면 더 좋을 것 입니다.

비록 사전이 없이 코퍼스만 가지고 피쳐(feature)를 추출하여 단어를 벡터로 만든다면, 워드넷의 정교한 지식은 이용할 수 없겠지만, 훨씬 더 간단한 작업이 될 것 입니다. 더욱이 코퍼스의 크기가 커질수록 추출된 특징들은 점점 더 정확해 질 것이고, 피쳐 벡터는 더욱 정확해 질 것 입니다. 피쳐 벡터가 추출 된 이후에는 코사인 유사도(cosine similarity)나 L2 디스턴스 등의 매트릭(metric)을 통해서 유사도를 계산할 수 있습니다.

하지만, 이전에 보았듯이, 단어 사전의 크기가 30,000~50,000이 넘는 현실에서 이렇게 추출된 피쳐 벡터의 차원은 단어 사전의 크기와 맞먹습니다. 단어 대신 문서를 피쳐로 사용하더라도 주어진 문서의 숫자만큼 벡터의 차원이 만들어질 것 입니다. 벡터가 큰 문제는 차치하고서라도, 더 큰 문제는 그 차원의 대부분의 값들이 0으로 채워져 있다는 것 입니다. 즉, 각 차원에 숫자가 나타나는 경우는 0이 나타나는 경우에 비해서 현저히 적습니다. 따라서 이런 0으로 가득한 희소(sparse) 벡터를 통해 무엇인가 배우고자 할 때에 큰 장애로 작용합니다. 특히, 앞서 배운 코사인 유사도의 경우에는 직교(orthogonoal)한 경우가 생겨 코사인 유사도의 값을 0으로 만들기 쉽습니다. 즉, 정확한 유사도를 구하기 어렵습니다.

이러한 희소성(sparsity) 문제는 자연어처리의 가장 큰 특징 입니다. 단어는 discrete한 심볼로 이루어져 있기 때문입니다. 따라서 전통적인 자연어처리에서는 이러한 희소성 문제로 인해서 정말 큰 어려움을 겪었습니다. 사실 많은 분들이 잘 알고 있듯이, 요즘 딥러닝에서는 단어의 피쳐 벡터를 이런 희소 벡터로 만들기보단 임베딩 기법을 통해 덴스(dense) 벡터로 만들어 사용 합니다. 이런 차이점이 바로 딥러닝에서의 자연어처리가 전통적인 방식의 자연어처리에 비해서 성능이 월등한 이유 입니다. 이제 우리는 앞으로 다룰 챕터에서 딥러닝을 통해 이런 자연어처리의 문제를 잘 해결하는 방법들을 소개해보고자 합니다.
