# 정리

이번 챕터에서는 지난 챕터에서 다룬 번역의 성능을 좀 더 향상 시킬 수 있는 방법들에 대해서 다루었습니다. 뉴럴 네트워크는 데이터가 많을수록 그 성능이 향상됩니다. 따라서 번역과 같은 sequence-to-sequece를 활용한 문제를 풀고자 할 때, 훈련 데이터인 병렬 코퍼스가 많으면 많을수록 좋습니다. 하지만 병렬 코퍼스는 수집이 매우 힘들고 제한적입니다. 따라서 이런 제한적인 상황에서 단방향 코퍼스(monolingual corpus)를 활용하여 번역의 성능을 향상 시키는 방법에 대해서 다루었습니다. 단방향 코퍼스를 활용한 성능 향상은 매우 큰 연구주제이며 앞으로 소개할 챕터들에서도 중점적으로 다루어질 예정이니, 기술의 발전에 대해서 주목하면 재미있게 읽을 수 있을 것 입니다.

또한 트랜스포머(Transformer)라는 모델 구조를 소개함으로써, 우리는 sequence-to-sequence가 다양하게 구현 가능함을 알 수 있었습니다. 특히 트랜스포머는 어텐션만을 사용하여 sequence-to-sequence를 구현함으로써, 속도와 성능의 두 마리 토끼를 모두 잡은 훌륭한 사례입니다. 트랜스포머는 이런 장점 때문에 현재 자연어처리 및 자연어생성에서 매우 중요하게 쓰이고 있으며, 앞으로도 그 역할이 더욱 기대 됩니다.

비록 번역에 국한하여 설명하였지만, 시퀀스 데이터를 입력으로 받아서 다른 도메인의 시퀀스 데이터로 출력하는 문제에 대해서도 대부분 적용 할 수 있을 것 입니다. 독자분들도 자신이 풀고자 하는 문제에 대해서 어떻게 적용할 수 있을지 고민하며 읽는다면 훨씬 더 도움이 될 것 같습니다.
