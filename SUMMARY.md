# Summary

* [소개글](README.md)
* [Index](index_list.md)
* [Temporal Note](tmp_note.md)
* [딥러닝을 활용한 자연어처리](nlp-with-deeplearning/cover.md)
  * [서문](nlp-with-deeplearning/intro.md)
  * [딥러닝의 역사](nlp-with-deeplearning/deeplearning.md)
  * [왜 자연어처리는 어려울까](nlp-with-deeplearning/why-nlp-difficult.md)
  * [왜 한국어 자연어처리는 더욱 어려울까](nlp-with-deeplearning/korean-is-hell.md)
  * [최근 추세](nlp-with-deeplearning/trends.md)
* [기초 수학](basic-math/cover.md)
  * [서문](basic-math/intro.md)
  * [랜덤 변수와 확률 분포](basic-math/prob-dist.md)
  * [쉬어가기: 몬티홀 문제](basic-math/monty-hall.md)
  * [기대값과 샘플링](basic-math/sampling.md)
  * [Maximum Likelihood Estimation](basic-math/mle.md)
  * [정보이론](basic-math/information.md)
  * [쉬어가기: Mean Square Error (MSE)](basic-math/mse.md)
  * [정리](basic-math/conclusion.md)
* [Hello PyTorch](pytorch-intro/cover.md)
  * [준비](pytorch-intro/intro.md)
  * [소개 및 설치](pytorch-intro/how-to-install.md)
  * [짧은 튜토리얼](pytorch-intro/hello-pytorch.md)
* [전처리](preprocessing/cover.md)
  * [들어가기에 앞서](preprocessing/intro.md)
  * [코퍼스 수집](preprocessing/collecting-corpus.md)
  * [코퍼스 정제](preprocessing/cleaning-corpus.md)
  * [분절하기 (형태소 분석)](preprocessing/tokenization.md)
  * [병렬 코퍼스 만들기](preprocessing/align.md)
  * [서브워드 분절하기](preprocessing/bpe.md)
  * [분절 복원하기](preprocessing/detokenization.md)
  * [토치텍스트](preprocessing/torchtext.md)
* [의미: 유사성과 모호성](word-senses/cover.md)
  * [소개](word-senses/intro.md)
  * [One-hot 인코딩](word-senses/one-hot-encoding.md)
  * [워드넷](word-senses/wordnet.md)
  * [피쳐란](word-senses/feature.md)
  * [피쳐 추출하기: TF-IDF](word-senses/tf-idf.md)
  * [피쳐 벡터 만들기](word-senses/vectorization.md)
  * [벡터 유사도 구하기](word-senses/similarity.md)
  * [단어 중의성 해소](word-senses/wsd.md)
  * [Selectional Preference](word-senses/selectional-preference.md)
  * [정리](word-senses/conclusion.md)
* [워드 임베딩](word-embedding-vector/cover.md)
  * [서문](word-embedding-vector/intro.md)
  * [차원 축소](word-embedding-vector/dimension-reduction.md)
  * [흔한 오해](word-embedding-vector/myth.md)
  * [Word2Vec](word-embedding-vector/word2vec.md)
  * [GloVe](word-embedding-vector/glove.md)
  * [예제 코드](word-embedding-vector/example.md)
  * [정리](word-embedding-vector/conclusion.md)
* [시퀀스 모델링](sequential-modeling/cover.md)
  * [서문](sequential-modeling/intro.md)
  * [Recurrent Neural Network](sequential-modeling/rnn.md)
  * [Long Short Term Memory](sequential-modeling/lstm.md)
  * [Gated Recurrent Unit](sequential-modeling/gru.md)
  * [그래디언트 클리핑](sequential-modeling/gradient-clipping.md)
  * [정리](sequential-modeling/conclusion.md)
* [텍스트 분류](text-classification/cover.md)
  * [서문](text-classification/intro.md)
  * [나이브 베이즈를 활용하기](text-classification/naive-bayes.md)
  * [흔한 오해 2](text-classification/myth.md)
  * [RNN을 활용하기](text-classification/rnn.md)
  * [CNN을 활용하기](text-classification/cnn.md)
  * [정리](text-classification/conclusion.md)
* [언어 모델링](language-modeling/cover.md)
  * [서문](language-modeling/intro.md)
  * [n-gram](language-modeling/n-gram.md)
  * [Perpexity](language-modeling/perpexity.md)
  * [n-gram 예제](language-modeling/srilm.md)
  * [뉴럴네트워크 언어 모델링](language-modeling/nnlm.md)
  * [활용 분야](language-modeling/application.md)
  * [정리](language-modeling/conclusion.md)
* [신경망 기계번역](neural-machine-translation/cover.md)
  * [서문](neural-machine-translation/intro.md)
  * [Sequence-to-Sequence](neural-machine-translation/seq2seq.md)
  * [Attention](neural-machine-translation/attention.md)
  * [Input Feeding](neural-machine-translation/input-feeding.md)
  * [Auto-regressive 속성과 Teacher Forcing](neural-machine-translation/teacher-forcing.md)
  * [탐색(추론)](neural-machine-translation/beam-search.md)
  * [성능 평가 방법](neural-machine-translation/eval.md)
  * [정리](neural-machine-translation/conclusion.md)
* [신경망 기계번역 심화 주제](adv-nmt/cover.md)
  * [다국어 번역](adv-nmt/multilingual-nmt.md)
  * [단방향 코퍼스를 활용하기](adv-nmt/monolingual-corpus.md)
  * [트랜스포머](adv-nmt/transformer.md)
  * [정리](adv-nmt/conclusion.md)
* [강화학습을 활용한 자연어생성](reinforcement-learning/cover.md)
  * [서문](reinforcement-learning/intro.md)
  * [강화학습 기초](reinforcement-learning/rl_basics.md)
  * [폴리시 그래디언트](reinforcement-learning/policy-gradient.md)
  * [자연어생성에서의 강화학습의 특성](reinforcement-learning/characteristic.md)
  * [강화학습을 활용한 지도학습](reinforcement-learning/supervised-nmt.md)
  * [강화학습을 활용한 비지도학습](reinforcement-learning/unsupervised-nmt.md)
  * [정리](reinforcement-learning/conclusion.md)
* [듀얼리티 활용하기](duality/cover.md)
  * [듀얼리티란](duality/intro.md)
  * [듀얼리티를 활용한 지도학습](duality/dsl.md)
  * [듀얼리티를 활용한 비지도학습](duality/dul.md)
  * [쉬어가기: Back-translation을 재해석 하기](duality/back_translation.md)
  * [정리](duality/conclusion.md)
* [서비스 만들기](productization/cover.md)
  * [파이프라인](productization/pipeline.md)
  * [구글의 신경망 기계번역](productization/gnmt.md)
  * [에딘버러 대학의 신경망 기계번역](productization/nematus.md)
  * [마이크로소프트의 신경망 기계번역](productization/microsoft.md)
* [이 책을 마치며](epilogue.md)
* [References](references.md)