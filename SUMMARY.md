# Summary

* [소개글](README.md)
* [서문](preface.md)
* [Index](index_list.md)
* [딥러닝을 활용한 자연어처리](01-introduction/00-cover.md)
  * [서문](01-introduction/01-intro.md)
  * [딥러닝의 역사](01-introduction/02-deeplearning.md)
  * [왜 자연어처리는 어려울까](01-introduction/03-why-nlp-difficult.md)
  * [왜 한국어 자연어처리는 더욱 어려울까](01-introduction/04-korean-is-hell.md)
  * [최근 추세](01-introduction/05-trends.md)
* [기초 수학](02-basic_math/00-cover.md)
  * [서문](02-basic_math/01-intro.md)
  * [랜덤 변수와 확률 분포](02-basic_math/02-prob-dist.md)
  * [쉬어가기: 몬티홀 문제](02-basic_math/03-monty-hall.md)
  * [기대값과 샘플링](02-basic_math/04-sampling.md)
  * [Maximum Likelihood Estimation](02-basic_math/05-mle.md)
  * [정보이론](02-basic_math/06-information.md)
  * [쉬어가기: Mean Square Error (MSE)](02-basic_math/07-mse.md)
  * [정리](02-basic_math/08-conclusion.md)
* [Hello PyTorch](03-pytorch_tutorial/00-cover.md)
  * [준비](03-pytorch_tutorial/01-intro.md)
  * [소개 및 설치](03-pytorch_tutorial/02-how-to-install.md)
  * [짧은 튜토리얼](03-pytorch_tutorial/03-hello-pytorch.md)
* [전처리](04-preprocessing/00-cover.md)
  * [들어가기에 앞서](04-preprocessing/01-intro.md)
  * [코퍼스 수집](04-preprocessing/02-collecting-corpus.md)
  * [코퍼스 정제](04-preprocessing/03-cleaning-corpus.md)
  * [분절하기 (형태소 분석)](04-preprocessing/04-tokenization.md)
  * [병렬 코퍼스 만들기](04-preprocessing/05-align.md)
  * [서브워드 분절하기](04-preprocessing/06-bpe.md)
  * [분절 복원하기](04-preprocessing/07-detokenization.md)
  * [토치텍스트](04-preprocessing/08-torchtext.md)
* [의미: 유사성과 모호성](05-word_senses/00-cover.md)
  * [소개](05-word_senses/01-intro.md)
  * [One-hot 인코딩](05-word_senses/02-one-hot-encoding.md)
  * [워드넷](05-word_senses/03-wordnet.md)
  * [피쳐란](05-word_senses/04-feature.md)
  * [피쳐 추출하기: TF-IDF](05-word_senses/05-tf-idf.md)
  * [피쳐 벡터 만들기](05-word_senses/06-vectorization.md)
  * [벡터 유사도 구하기](05-word_senses/07-similarity.md)
  * [단어 중의성 해소](05-word_senses/08-wsd.md)
  * [Selectional Preference](05-word_senses/09-selectional-preference.md)
  * [정리](05-word_senses/10-conclusion.md)
* [워드 임베딩](06-word_embedding/00-cover.md)
  * [서문](06-word_embedding/01-intro.md)
  * [차원 축소](06-word_embedding/02-dimension-reduction.md)
  * [흔한 오해](06-word_embedding/03-myth.md)
  * [Word2Vec](06-word_embedding/04-word2vec.md)
  * [GloVe](06-word_embedding/05-glove.md)
  * [예제](06-word_embedding/06-example.md)
  * [정리](06-word_embedding/07-conclusion.md)
* [시퀀스 모델링](07-sequential_modeling/00-cover.md)
  * [서문](07-sequential_modeling/01-intro.md)
  * [Recurrent Neural Network](07-sequential_modeling/02-rnn.md)
  * [Long Short Term Memory](07-sequential_modeling/03-lstm.md)
  * [Gated Recurrent Unit](07-sequential_modeling/04-gru.md)
  * [그래디언트 클리핑](07-sequential_modeling/05-gradient-clipping.md)
  * [정리](07-sequential_modeling/06-conclusion.md)
* [텍스트 분류](08-text_classification/00-cover.md)
  * [서문](08-text_classification/01-intro.md)
  * [나이브 베이즈를 활용하기](08-text_classification/02-naive-bayes.md)
  * [흔한 오해 2](08-text_classification/03-myth.md)
  * [RNN을 활용하기](08-text_classification/04-rnn.md)
  * [CNN을 활용하기](08-text_classification/05-cnn.md)
  * [쉬어가기: 멀티 레이블 분류](08-text_classification/06-multi_classification.md)
  * [정리](08-text_classification/07-conclusion.md)
* [언어 모델링](09-language_modeling/00-cover.md)
  * [서문](09-language_modeling/01-intro.md)
  * [n-gram](09-language_modeling/02-n-gram.md)
  * [Perpexity](09-language_modeling/03-perpexity.md)
  * [n-gram 예제](09-language_modeling/04-srilm.md)
  * [뉴럴네트워크 언어 모델링](09-language_modeling/05-nnlm.md)
  * [활용 분야](09-language_modeling/06-application.md)
  * [정리](09-language_modeling/07-conclusion.md)
* [신경망 기계번역](10-neural_machine_translation/00-cover.md)
  * [서문](10-neural_machine_translation/01-intro.md)
  * [Sequence-to-Sequence](10-neural_machine_translation/02-seq2seq.md)
  * [Attention](10-neural_machine_translation/03-attention.md)
  * [Input Feeding](10-neural_machine_translation/04-input-feeding.md)
  * [Auto-regressive 속성과 Teacher Forcing](10-neural_machine_translation/05-teacher-forcing.md)
  * [탐색(추론)](10-neural_machine_translation/06-beam-search.md)
  * [성능 평가 방법](10-neural_machine_translation/07-eval.md)
  * [정리](10-neural_machine_translation/08-conclusion.md)
* [신경망 기계번역 심화 주제](11-adv_neural_machine_translation/00-cover.md)
  * [다국어 번역](11-adv_neural_machine_translation/01-multilingual-nmt.md)
  * [단방향 코퍼스를 활용하기](11-adv_neural_machine_translation/02-monolingual-corpus.md)
  * [트랜스포머](11-adv_neural_machine_translation/03-transformer.md)
  * [정리](11-adv_neural_machine_translation/04-conclusion.md)
* [강화학습을 활용한 자연어생성](12-reinforcement_learning/00-cover.md)
  * [서문](12-reinforcement_learning/01-intro.md)
  * [강화학습 기초](12-reinforcement_learning/02-rl_basics.md)
  * [폴리시 그래디언트](12-reinforcement_learning/03-policy-gradient.md)
  * [자연어생성에서의 강화학습의 특성](12-reinforcement_learning/04-characteristic.md)
  * [강화학습을 활용한 지도학습](12-reinforcement_learning/05-supervised-nmt.md)
  * [강화학습을 활용한 비지도학습](12-reinforcement_learning/06-unsupervised-nmt.md)
  * [정리](12-reinforcement_learning/07-conclusion.md)
* [듀얼리티 활용하기](13-duality/00-cover.md)
  * [듀얼리티란](13-duality/01-intro.md)
  * [듀얼리티를 활용한 지도학습](13-duality/02-dsl.md)
  * [듀얼리티를 활용한 비지도학습](13-duality/03-dul.md)
  * [쉬어가기: Back-translation을 재해석 하기](13-duality/04-back_translation.md)
  * [정리](13-duality/05-conclusion.md)
* [서비스 만들기](14-productization/00-cover.md)
  * [파이프라인](14-productization/01-pipeline.md)
  * [구글의 신경망 기계번역](14-productization/02-gnmt.md)
  * [에딘버러 대학의 신경망 기계번역](14-productization/03-nematus.md)
  * [마이크로소프트의 신경망 기계번역](14-productization/04-microsoft.md)
* [전이학습 활용하기](15-transfer_learning/00-cover.md)
  * [전이학습이란](15-transfer_learning/01-intro.md)
  * [기존의 방법](15-transfer_learning/02-previous_work.md)
  * [ELMo](15-transfer_learning/03-elmo.md)
  * [BERT](15-transfer_learning/04-bert.md)
  * [기계번역에 적용하기](15-transfer_learning/05-machine_translation.md)
  * [정리](15-transfer_learning/06-conclusion.md)
* [이 책을 마치며](epilogue.md)
* [References](references.md)
