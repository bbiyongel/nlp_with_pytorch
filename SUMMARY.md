# Summary

* [소개글](README.md)
* [서문](preface.md)
* [Index](index_list.md)
* [딥러닝을 활용한 자연어처리](1-introduction/cover.md)
  * [서문](1-introduction/intro.md)
  * [딥러닝의 역사](1-introduction/deeplearning.md)
  * [왜 자연어처리는 어려울까](1-introduction/why-nlp-difficult.md)
  * [왜 한국어 자연어처리는 더욱 어려울까](1-introduction/korean-is-hell.md)
  * [최근 추세](1-introduction/trends.md)
* [기초 수학](2-basic_math/cover.md)
  * [서문](2-basic_math/intro.md)
  * [랜덤 변수와 확률 분포](2-basic_math/prob-dist.md)
  * [쉬어가기: 몬티홀 문제](2-basic_math/monty-hall.md)
  * [기대값과 샘플링](2-basic_math/sampling.md)
  * [Maximum Likelihood Estimation](2-basic_math/mle.md)
  * [정보이론](2-basic_math/information.md)
  * [쉬어가기: Mean Square Error (MSE)](2-basic_math/mse.md)
  * [정리](2-basic_math/conclusion.md)
* [Hello PyTorch](3-pytorch_tutorial/cover.md)
  * [준비](3-pytorch_tutorial/intro.md)
  * [소개 및 설치](3-pytorch_tutorial/how-to-install.md)
  * [짧은 튜토리얼](3-pytorch_tutorial/hello-pytorch.md)
* [전처리](4-preprocessing/cover.md)
  * [들어가기에 앞서](4-preprocessing/intro.md)
  * [코퍼스 수집](4-preprocessing/collecting-corpus.md)
  * [코퍼스 정제](4-preprocessing/cleaning-corpus.md)
  * [분절하기 (형태소 분석)](4-preprocessing/tokenization.md)
  * [병렬 코퍼스 만들기](4-preprocessing/align.md)
  * [서브워드 분절하기](4-preprocessing/bpe.md)
  * [분절 복원하기](4-preprocessing/detokenization.md)
  * [토치텍스트](4-preprocessing/torchtext.md)
* [의미: 유사성과 모호성](5-word_senses/cover.md)
  * [소개](5-word_senses/intro.md)
  * [One-hot 인코딩](5-word_senses/one-hot-encoding.md)
  * [워드넷](5-word_senses/wordnet.md)
  * [피쳐란](5-word_senses/feature.md)
  * [피쳐 추출하기: TF-IDF](5-word_senses/tf-idf.md)
  * [피쳐 벡터 만들기](5-word_senses/vectorization.md)
  * [벡터 유사도 구하기](5-word_senses/similarity.md)
  * [단어 중의성 해소](5-word_senses/wsd.md)
  * [Selectional Preference](5-word_senses/selectional-preference.md)
  * [정리](5-word_senses/conclusion.md)
* [워드 임베딩](6-word_embedding/cover.md)
  * [서문](6-word_embedding/intro.md)
  * [차원 축소](6-word_embedding/dimension-reduction.md)
  * [흔한 오해](6-word_embedding/myth.md)
  * [Word2Vec](6-word_embedding/word2vec.md)
  * [GloVe](6-word_embedding/glove.md)
  * [예제](6-word_embedding/example.md)
  * [정리](6-word_embedding/conclusion.md)
* [시퀀스 모델링](7-sequential_modeling/cover.md)
  * [서문](7-sequential_modeling/intro.md)
  * [Recurrent Neural Network](7-sequential_modeling/rnn.md)
  * [Long Short Term Memory](7-sequential_modeling/lstm.md)
  * [Gated Recurrent Unit](7-sequential_modeling/gru.md)
  * [그래디언트 클리핑](7-sequential_modeling/gradient-clipping.md)
  * [정리](7-sequential_modeling/conclusion.md)
* [텍스트 분류](8-text_classification/cover.md)
  * [서문](8-text_classification/intro.md)
  * [나이브 베이즈를 활용하기](8-text_classification/naive-bayes.md)
  * [흔한 오해 2](8-text_classification/myth.md)
  * [RNN을 활용하기](8-text_classification/rnn.md)
  * [CNN을 활용하기](8-text_classification/cnn.md)
  * [정리](8-text_classification/conclusion.md)
* [언어 모델링](9-language_modeling/cover.md)
  * [서문](9-language_modeling/intro.md)
  * [n-gram](9-language_modeling/n-gram.md)
  * [Perpexity](9-language_modeling/perpexity.md)
  * [n-gram 예제](9-language_modeling/srilm.md)
  * [뉴럴네트워크 언어 모델링](9-language_modeling/nnlm.md)
  * [활용 분야](9-language_modeling/application.md)
  * [정리](9-language_modeling/conclusion.md)
* [신경망 기계번역](10-neural_machine_translation/cover.md)
  * [서문](10-neural_machine_translation/intro.md)
  * [Sequence-to-Sequence](10-neural_machine_translation/seq2seq.md)
  * [Attention](10-neural_machine_translation/attention.md)
  * [Input Feeding](10-neural_machine_translation/input-feeding.md)
  * [Auto-regressive 속성과 Teacher Forcing](10-neural_machine_translation/teacher-forcing.md)
  * [탐색(추론)](10-neural_machine_translation/beam-search.md)
  * [성능 평가 방법](10-neural_machine_translation/eval.md)
  * [정리](10-neural_machine_translation/conclusion.md)
* [신경망 기계번역 심화 주제](adv-nmt/cover.md)
  * [다국어 번역](adv-nmt/multilingual-nmt.md)
  * [단방향 코퍼스를 활용하기](adv-nmt/monolingual-corpus.md)
  * [트랜스포머](adv-nmt/transformer.md)
  * [정리](adv-nmt/conclusion.md)
* [강화학습을 활용한 자연어생성](reinforcement-learning/cover.md)
  * [서문](reinforcement-learning/intro.md)
  * [강화학습 기초](reinforcement-learning/rl_basics.md)
  * [폴리시 그래디언트](reinforcement-learning/policy-gradient.md)
  * [자연어생성에서의 강화학습의 특성](reinforcement-learning/characteristic.md)
  * [강화학습을 활용한 지도학습](reinforcement-learning/supervised-nmt.md)
  * [강화학습을 활용한 비지도학습](reinforcement-learning/unsupervised-nmt.md)
  * [정리](reinforcement-learning/conclusion.md)
* [듀얼리티 활용하기](duality/cover.md)
  * [듀얼리티란](duality/intro.md)
  * [듀얼리티를 활용한 지도학습](duality/dsl.md)
  * [듀얼리티를 활용한 비지도학습](duality/dul.md)
  * [쉬어가기: Back-translation을 재해석 하기](duality/back_translation.md)
  * [정리](duality/conclusion.md)
* [서비스 만들기](14_productization/cover.md)
  * [파이프라인](14_productization/pipeline.md)
  * [구글의 신경망 기계번역](14_productization/gnmt.md)
  * [에딘버러 대학의 신경망 기계번역](14_productization/nematus.md)
  * [마이크로소프트의 신경망 기계번역](14_productization/microsoft.md)
* [이 책을 마치며](epilogue.md)
* [References](references.md)
