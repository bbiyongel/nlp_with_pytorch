# 언어 모델링 (Language Modeling)

## 소개

이전 챕터까지 우리는 단어 또는 문장을 입력으로 받아서, 어떤 값 또는 클래스로 분류하는 방법에 대해서 다루었습니다. 그 값을 통해 해당 단어 또는 문장을 분류하기도 하고 군집을 형성(클러스터링, clustering) 할 수도 있었습니다. 이러한 방법들도 매우 쓰임새가 많고, 정말 중요합니다. 하지만 우리는 여기서 더 나아가 뉴럴 네트워크로 하여금 필요에 따라 자연스러운 문장을 만들어내도록 하는 방법에 대해 다루어 보도록 하겠습니다.

언어 모델(Language Model)은 문장의 확률을 나타내는 모델 입니다. 즉, 우리는 언어 모델을 통해 문장 자체의 출현 확률을 예측 하거나, 이전 단어들이 주어졌을 때 다음 단어를 예측할 수 있으며, 결과적으로 우리는 주어진 문장이 얼마나 자연스러운 유창한(fluent) 표현인지 계산 할 수 있게 됩니다. 예를 들어 우리는 아래와 같은 문장이 주어졌을 때, 빈칸을 어렵지 않게 메꿀 수 있습니다.

|번호|버스 정류장에서 방금 버스를 _______.|
|:-:|:-:|
|1|사랑해|
|2|고양이|
|3|놓쳤다|
|4|사고남|

우리는 정답이 3번 "놓쳤다"라고 쉽게 맞출 수 있습니다. 4번 "사고남"의 경우에는 앞 단어가 "버스를" 대신에 "버스가"였다면 정답이 될 수도 있었습니다. 하지만 4번을 정답이라고 할 경우에는 뜻을 전달하는데는 큰 무리가 없겠지만, 뭔가 어색함이 느껴지는 문장이 될 겁니다. 

|번호|문장|
|-|-|
|1|저는 어제 점심을 먹었습니다.|
|2|저는 2018년 4월 26일 점심을 먹었습니다.|

위의 두 문장이 주어졌다고 해 보겠습니다. 우리가 살아가면서 1번 문장을 접할 기회는, 2번 문장을 접할 기회보다 훨씬 많을 겁니다. 수많은 단어들이 있고, 그 단어들간의 조합은 더욱 많이 존재 합니다. 그러한 조합들은 동등한 확률을 갖기보다는 자주 나타나는 단어나 표현(단어의 조합)이 훨씬 높은 확률로 나타날 겁니다.

이렇게 우리는 살아오면서 수많은 문장을 접하였고, 우리의 머릿속에는 단어와 단어 사이의 확률이 우리도 모르게 학습되어 있습니다. 덕분에 누군가와 대화를 하다가 몇 단어 정도는 완벽하게 알아듣지 못하여도 대화에 지장이 없습니다. <comment>물론 여기에는 문맥 정보를 이용 하는 것도 큰 도움이 됩니다.</comment> 이와 같이 꼭 자연어처리 분야가 아니더라도 음성인식(Speech Recognition)이나 문자인식(Optical Character Recognition, OCR)에 있어서도 언어모델은 큰 역할을 수행합니다. 

언어모델을 학습하고 구성하기 위해서, 우리는 인터넷이나 도서에서 많은 문장들을 (대개 수십만에서 수억 까지) 수집하여 단어와 단어 사이의 출현 빈도를 세어 확률을 계산합니다. 이 과정의 궁극적인 목표는 우리가 일상 생활에서 사용하는 ground-truth 언어(문장)의 분포를 정확하게 근사(approximate)하는데 있습니다. 또한, 만약 의료 분야의 음성인식기 제작과 같은 특정한 목표를 가지고 있다면, 해당 분야(domain)의 문장의 분포를 파악하기 위해서 해당 분야의 코퍼스를 수집하기도 합니다.

## 다시 한번, 지옥불 난이도 한국어

우리는 언어들의 구조적 특성에 따라서 여러 갈래로 언어를 분류합니다. 이전 챕터에서 다루었듯이 한국어는 대표적인 교착어입니다. 그리고 영어는 고립어(+굴절어)의 특성을 띄고, 중국어는 고립어로 분류합니다. 교착어의 특성상, 단어의 의미 또는 역할은 어순에 의해 결정되기 보단, 단어에 부착되는 어미와 같은 접사에 의해 그 역할이 결정됩니다. 따라서 같은 의미의 단어라 할지라도 붙는 접사에 따라 단어의 형태가 달라지게 되어 단어의 수가 늘어나게 됩니다. "버스+가", "버스+를", "버스+에", "버스+로" 등 같은 "버스"라도 뒤에 붙는 접사에 따라 다른 단어가 됩니다. 즉, 단어의 어순이 중요하지 않기 때문에 (또는 생략 가능하기 때문에), 단어와 단어 사이의 확률을 계산하는데 불리하게 작용할 수 있습니다.

|번호|문장|
|-|-|
|1|나는 학교에 갑니다 버스를 타고 .|
|2|나는 버스를 타고 학교에 갑니다 .|
|3|버스를 타고 나는 학교에 갑니다 .|
|4|(나는) 버스를 타고 학교에 갑니다 .|

위의 네 문장 모두 같은 의미의 표현이고 사용 된 단어들도 같지만, 어순이 다르기 때문에, 단어와 단어 사이의 확률을 정의하는데 있어서 혼란이 가중됩니다. 같은 의미의 문장을 표현하기 위해서 '타고' 다음에 나타날 수 있는 단어들은 '.', '학교에', '나는' 3개이기 때문에, 확률이 쉽게 말해 퍼지는 현상이 생기게 됩니다. 다음 단어를 예측(또는 찍을때) 훨씬 헷갈릴 가능성이 높아질 것 입니다. 이에 반해 영어나 기타 라틴어 기반 언어들은 좀 더 어순에 있어서 규칙적이므로 좀 더 유리합니다.

더군다나, 한국어는 교착어의 특성상 접사가 붙어 단어의 의미와 역할이 결정되기 때문에 같은 '학교'라는 단어가 '학교+에', '학교+로', '학교+를', '학교+가', ... 등 수많은 단어로 파생되어 만들어질 수 있습니다. 따라서 사실 어미를 따로 분리해주지 않으면 어휘의 수가 기하급수적으로 늘어나게 되어 희소성(sparseness)이 더욱 높아질 수 있어 더욱 문제 해결이 어려워질 수 있습니다.

## 문장의 확률 표현

주어진 문장에 대해서 어떻게 확률을 구할 수 있을까요? $w_1$, $w_2$라는 2개의 단어가 한 문장 안에 순서대로 나타났을 때, 이 문장의 확률은 아래와 같이 표현 가능합니다.

$$
P(w_1, w_2)
$$

우리는 이 수식을 베이즈 정리(Bayes Theorem)에 따라 조건부 확률(Conditional Probability)로 표현할 수 있습니다.

$$
\begin{aligned}
P(w_1, w_2) &= P(w_1)P(w_2|w_1) \\
&\text{because} \\
P(w_2|w_1)&=\frac{P(w_1,w_2)}{P(w_1)}.
\end{aligned}
$$

좀 더 나아가서 체인룰(연쇄법칙, Chain Rule)을 통해, 여러 단어가 나타날 확률을 분리하여 표현할 수 있습니다.

$$
P(w_1,w_2,\cdots,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1, w_2)\cdots P(w_n|w_1,w_2,\cdots,w_{n-1})
$$

여기서 체인룰이란, 조건부 확률을 사용하여 결합 확률을 계산 하는 방법으로, 아래와 같이 유도 할 수 있습니다.

$$
\begin{aligned}
P(A,B,C,D)&=P(D|A,B,C)P(A,B,C) \\
&=P(D|A,B,C)P(C|A,B)P(A,B) \\
&=P(D|A,B,C)P(C|A,B)P(B|A)P(A)
\end{aligned}
$$ 

다시 n개의 단어가 주어졌을때 문장의 확률을 나타낸 수식에서 우항을 해석해보면, $w_1$가 나타날 확률과 $w_1$가 주어졌을 때 $w_2$가 나타날 확률, $w_1, w_2$가 주어졌을 때 $w_3$가 주어졌을 확률, $w_1, w_2,\cdots,w_{n-1}$이 주어졌을 때 $w_n$가 나타날 확률을 곱하는 것을 알 수 있습니다. 이로써 우리는 언어모델을 활용하여 문장에 대한 확률 뿐만 아니라, 단어와 단어 사이의 확률도 정의 할 수 있습니다. 우리는 위의 수식을 일반화하여 아래와 같이 표현할 수 있습니다.

$$
P(W)=\prod_{i=1}^{n}{P(w_i|w_{<i})}
$$

또는 로그 확률(log probability)로 표현하여 곱셈 대신 덧셈으로 표현할 수 있습니다. 참고로, 문장이 길어지게 된다면 당연히 확률에 대한 곱셈이 거듭되면서 확률이 매우 작아지게 되어 정확한 계산 또는 표현이 힘들어지게 됩니다. <comment>또한, 곱셈 연산보다 덧셈 연산이 더 빠릅니다.</comment> 따라서 우리는 로그를 취하여 덧셈으로 바꾸어 더 나은 조건을 취할 수 있습니다. 

$$
\log{P(W)}=\sum_{i=1}^{n}{\log{P(w_i|w_{<i})}}
$$

이제 우리는 실제 예제를 가지고 표현 해 보도록 하겠습니다. 코퍼스 $\mathcal{C}=\{s_1,s_2,\cdots,s_n\}$에서 $i$번째 문장 $s_i = \{\text{BOS}, \text{나는}, \text{학교에}, \text{갑니다}, \text{EOS}\}$에 대한 확률을 체인룰을 통해 표현하면 아래와 같습니다.

$$
P(\text{BOS},\text{나는},\text{학교에},\text{갑니다},\text{EOS})=P(\text{BOS})P(\text{나는}|\text{BOS})P(\text{학교에}|\text{BOS},\text{나는})P(\text{갑니다}|\text{BOS},\text{나는},\text{학교에})P(\text{EOS}|\text{BOS},\cdots,\text{갑니다})
$$

여기서 BOS는 Beginning of Sentence(문장의 시작)라는 의미의 토큰이고, EOS는 End of sentence(문장의 종료)라는 의미의 토큰입니다. $P(\text{BOS})$의 경우에는 항상 문장의 시작에 오게 되므로 상수가 될 것 입니다. $P(\text{나는}|\text{BOS})$ 경우에는 문장의 시작 후 첫 단어로 "나는"이 올 확률을 나타내게 됩니다. 

이제 문장을 어떻게 확률로 나타내는지 알았으니, 확률을 직접 구하는 방법에 대해서 알아보겠습니다. 우리는 앞 챕터에서 문장을 수집하는 방법에 대해서 논의 했습니다. 수집한 말뭉치 내에서 직접 단어들의 출현 빈도를 계산함으로써, 우리가 원하는 확률을 추정할 수 있습니다. 예를 들어 아래의 확률은 다음과 같이 추정할 수 있습니다.

$$
P(\text{갑니다}|]\text{BOS},\text{나는},\text{학교에})\approx\frac{\text{Count}(\text{BOS},\text{나는},\text{학교에},\text{갑니다})}{\text{Count}(\text{BOS},\text{나는},\text{학교에})}
$$
