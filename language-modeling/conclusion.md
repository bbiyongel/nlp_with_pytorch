# 정리

이번 챕터에서 우리는 주어진 문장을 확률적으로 모델링 하는 방법을 이야기 하였습니다. 딥러닝 이전부터 자연어처리를 통한 문장에 대한 활용 능력의 필요성은 꾸준히 있었기 때문에, n-gram과 같은 방법들을 통해서 많은 곳에 활용되고 있었습니다. 하지만 n-gram과 같은 방식들은 여전히 단어를 discrete한 존재로 취극하였기 때문에, 희소성(sparsity) 문제를 해결하지 못하여, 일반화(generalization) 능력에서 많은 어려움을 겪었습니다. 이에 따라 마코프 가정(Markov assumption)부터 스무딩(smoothing)과 디스카운팅(discounting)을 통해서 n-gram의 단점을 보완하고자 하였지만, 근본적으로 n-gram은 출현빈도에 기반하기 때문에 완벽한 해결책이 될 수는 없었습니다.

하지만 뉴럴 네트워크를 통해서 언어모델링을 수행하게 되면, 일반화의 문제가 해결되게 됩니다. 뉴럴 네트워크는 비선형적 차원축소에 매우 뛰어난 성능을 가지고 있기 때문에, 희소한 단어들의 조합에 대해서도 효과적으로 차원축소하여, 기존의 훈련 데이터 내의 다른 단어 조합에 대해서 훌륭하게 유사도 비교 등을 수행할 수 있습니다. 이에 따라서 뉴럴 네트워크는 추론 수행 과정에서 처음 보는 시퀀스의 데이터가 주어지더라도 기존에 자신이 배운 것에 기반하여 나름 훌륭한 예측을 해낼 수 있습니다.

우리는 언어모델링이 정말 많은 분야에서 중요한 기초로써 다양하게 활용되고 있음을 배웠습니다. 따라서 뉴럴 네트워크를 통해 개선된 언어모델을 활용하여 앞으로 이어지는 챕터에서는 자연어 생성, 특히 번역에 대해서 다루고자 합니다.
