# 신경망 기계번역 심화 주제

![[Christopher Manning: Professor at Stanford Univ.](https://nlp.stanford.edu/manning/)](../assets/11-00-01.jpeg){ width=500px }

앞선 챕터에서 우리는 기게번역과 이를 위한 sequence-to-sequence 그리고 어텐션 기법에 대해 다루었습니다. 우리는 기본적인 자연어생성 기법으로도 이미 기존 딥러닝 이전의 기술을 훨씬 뛰어넘는 성능을 뽑아낼 수 있음을 보았습니다. 하지만 자연어생성 또는 기계번역을 훈련하기 위해서는 다량의 데이터 구축이 필요하고, 이 비용은 매우 비싸고 어렵습니다. 이번 챕터에서는 이러한 어려움을 해결하기 위한 방법들을 소개하고, 더 나아가 ‘Attention is All You Need’라는 제목으로 유명한 트랜스포머(Transformer) 모델에 대해서도 살펴보도록 합니다. 이를 통해 기계번역 또는 자연어생성 성능을 더욱 끌어올리기 위한 다양한 고급 기법들에 대해서 이야기 하고자 합니다.
