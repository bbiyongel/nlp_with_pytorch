# 신경망 기계번역 심화 주제

![[Christopher Manning: Professor at Stanford Univ.](https://nlp.stanford.edu/manning/)](../assets/11-00-01.jpeg){ width=500px }

앞선 챕터에서 우리는 기게번역과 이를 위한 sequence-to-sequence 그리고 어텐션 기법에 대해 깊이 있게 다루었습니다. 우리는 기본적인 sequence-to-sequence를 활용한 자연어생성 기법으로도 이미 기존 딥러닝 이전의 기술을 훨씬 뛰어넘는 성능을 뽑아낼 수 있음을 보았습니다. 하지만 자연어생성 또는 기계번역을 훈련하기 위해서는 다량의 데이터 구축이 필요하고, 이 비용은 매우 비싸고 어렵습니다. 따라서 병렬 코퍼스 이외에도 손쉽게 얻을 수 있는 단방향(monolingual) 코퍼스를 통해서도 기계번역의 성능을 향상하는 것은 매우 중요한 연구 주제 입니다. 이는 기계번역에서 뿐만 아니라 병렬 코퍼스가 필요한 자연어생성 문제 전반에 적용 될 수 있는 아주 유용한 주제 입니다. 이번 챕터에서는 이러한 자연어생성의 어려움을 해결하기 위한 방법들을 소개하고, 더 나아가 ‘Attention is All You Need’라는 제목으로 유명한 트랜스포머(Transformer) 모델에 대해서도 살펴보도록 합니다. 이를 통해 기계번역 또는 자연어생성 성능을 더욱 끌어올리기 위한 다양한 고급 기법들에 대해서 이야기 하고자 합니다.
