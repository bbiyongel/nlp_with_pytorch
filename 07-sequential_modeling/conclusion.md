# 정리

이 챕터에서는 RNN을 활용한 순서 정보를 가진 시퀀셜 데이터 또는 시계열 데이터를 학습하는 방법에 대해 배웠습니다. 기존의 뉴럴 네트워크와 달리 RNN은 이전 time-step의 자기자신을 참조하여 현재 자기자신의 상태를 결정합니다. 이에 따라 time-step마다 RNN의 네트워크 웨이트 파라미터는 공유 됩니다. 하지만 RNN은 그래디언트 소실(gradient vanishing)과 같은 문제가 잘 발생하기 때문에, 긴 시퀀스 데이터를 잘 처리하지 못하는 단점이 있었습니다.

LSTM과 GRU는 이전 RNN의 단점을 보완하여 여러가지 게이트(gate)들을 열고닫아 정보의 흐름을 조절함으로써, 좀 더 장기기억력에 더 나은 성능을 보여줍니다. 따라서 보통 RNN을 사용할 때는 LSTM을 가장 많이 사용하며, 사용자의 선호에 따라서 GRU를 사용하기도 합니다.

RNN의 back-propagation은 시간에 대해서도 이루어 집니다. 따라서 time-step이 많은 데이터일수록, time-step별 그래디언트가 더해져서 최종 그래디언트가 커지게 됩니다. 그래디언트가 클 때, 너무 큰 learning-rate를 사용하게 되면 해당 학습은 발산할 가능성이 높습니다. 따라서 우리는 그래디언트 클리핑을 통해서 그래디언트가 정해진 임계치보다 클 경우에, 방향은 유지한 채로 그래디언트의 크기를 임계치만큼 감소 시킵니다. 이에 따라 우리는 그래디언트의 최대 크기가 정해지게 되기 때문에 learning-rate 1과 같은 매우 큰 값도 학습에 사용할 수 있게 됩니다.

이번 챕터는 앞으로 소개할 챕터들의 가장 큰 기초가 되는 챕터입니다. 시퀀셜 데이터인 자연어처리의 문장을 다루고자 할 때는, RNN을 사용하는 것이 가장 기본이기 때문입니다. 앞으로 이야기할 텍스트 분류, 번역과 같은 문제들에서 RNN이 어떻게 활용되는지 주목해 주세요.
